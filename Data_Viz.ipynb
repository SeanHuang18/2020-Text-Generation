{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\keith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 1
    }
   ],
   "source": [
    "import data_generator\n",
    "\n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "D = data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ddca3d227590>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_batch() missing 1 required positional argument: 'self'"
     ],
     "ename": "TypeError",
     "evalue": "generate_batch() missing 1 required positional argument: 'self'",
     "output_type": "error"
    }
   ],
   "source": [
    "D.generate_batch(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"WordDataset.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "\n",
      "New\n",
      "\n",
      "York\n",
      "\n",
      "Post\n",
      "\n",
      "has\n",
      "\n",
      "learned\n",
      "\n",
      "that\n",
      "\n",
      "the\n",
      "\n",
      "woman\n",
      "\n",
      "accusing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    s = f.readline()\n",
    "    s.replace(\"\\n\", \" \")\n",
    "    print(s)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"WordDataset.txt\", \"r\")\n",
    "data = []\n",
    "articletext = \"\"\n",
    "for i in range(50):\n",
    "    s = f.readline()\n",
    "    s = s[:-1]\n",
    "    data.append(s)\n",
    "    articletext = articletext + \" \" + s\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'New', 'York', 'Post', 'has', 'learned', 'that', 'the', 'woman', 'accusing', 'IMF', 'boss', 'Dominique', 'Strauss-Kahn', 'of', 'sexual', 'assault', 'lived', 'in', 'an', 'apartment', 'exclusively', 'for', 'patients', 'with', 'HIV', 'and', 'AIDS.', 'From', 'the', 'Post:', 'The', 'hotel', 'maid,', 'a', 'West', 'African', 'immigrant,', 'has', 'occupied', 'the', 'fourth-floor', 'High', 'Bridge', 'pad', 'with', 'her', '15-year-old', 'daughter', 'since']\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_article = articletext.lower()\n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'new', 'york', 'post', 'has', 'learned', 'that', 'the', 'woman', 'accusing', 'imf', 'boss', 'dominique', 'strauss', 'kahn', 'of', 'sexual', 'assault', 'lived', 'in', 'an', 'apartment', 'exclusively', 'for', 'patients', 'with', 'hiv', 'and', 'aids', 'from', 'the', 'post', 'the', 'hotel', 'maid', 'a', 'west', 'african', 'immigrant', 'has', 'occupied', 'the', 'fourth', 'floor', 'high', 'bridge', 'pad', 'with', 'her', 'year', 'old', 'daughter', 'since']]\n"
     ]
    }
   ],
   "source": [
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(all_words, min_count=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': <gensim.models.keyedvectors.Vocab object at 0x7fd4880a80d0>, 'post': <gensim.models.keyedvectors.Vocab object at 0x7fd4880a82d0>, 'has': <gensim.models.keyedvectors.Vocab object at 0x7fd4880a8750>, 'with': <gensim.models.keyedvectors.Vocab object at 0x7fd4880a8290>}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = word2vec.wv.vocab\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = word2vec.wv['with']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.9757158e-03  1.1201568e-03 -9.5006137e-04  1.2618440e-03\n",
      "  2.7760344e-03 -4.6324693e-03 -4.0157796e-03  2.6071491e-03\n",
      "  1.9309936e-03 -3.0152767e-03  1.6532142e-03 -4.5209620e-04\n",
      " -9.9729281e-04  3.6155121e-04 -1.0506184e-03  4.4127647e-03\n",
      "  1.6917358e-03  1.9575734e-04  1.5167347e-03 -2.5260863e-03\n",
      "  3.7059977e-03  2.9814700e-03 -2.6468735e-03 -4.1453255e-04\n",
      " -1.9229174e-03 -3.2412482e-03  3.9280863e-03  2.8991266e-04\n",
      "  1.2503249e-03  3.7439242e-03 -4.8989174e-03  3.7739109e-03\n",
      "  3.9800717e-03  2.2297055e-03  2.8489970e-03  9.9437567e-04\n",
      " -1.3467791e-03  2.7653272e-03  2.1572500e-03  1.0282841e-03\n",
      " -2.6402066e-03  2.4500166e-04  2.4134098e-03 -1.4094921e-03\n",
      "  1.2794139e-03 -2.9167258e-03  3.3776548e-03  1.9189342e-03\n",
      " -4.8179571e-03  1.0212257e-03  2.4693897e-03 -1.5774863e-03\n",
      " -2.8253312e-03 -4.7459682e-03 -4.2783301e-03  1.5775380e-03\n",
      " -2.6343742e-03  2.8562464e-03 -1.3652543e-03 -4.6919305e-03\n",
      " -1.3807492e-03 -3.2645147e-03  3.4333405e-03 -1.0047904e-03\n",
      " -2.3658997e-03 -1.0918876e-03  3.3131405e-04 -3.0173431e-03\n",
      "  3.6274162e-03 -3.4085794e-03  5.7141697e-05  4.7699298e-04\n",
      " -6.4968999e-04 -3.5088051e-03 -2.9207524e-03 -3.7146113e-03\n",
      " -1.5782922e-03 -1.8664978e-03 -2.5655823e-03 -1.7777039e-03\n",
      " -3.9825551e-03 -4.1608936e-03  1.4069992e-03  1.5647486e-03\n",
      " -1.4775649e-03 -1.4923486e-03 -3.8031158e-03  2.6079873e-03\n",
      "  4.6561384e-03  2.8405513e-03  1.3495731e-03  2.3103629e-04\n",
      "  4.7246055e-03 -3.6365411e-03 -3.3665744e-03  4.3943501e-03\n",
      "  3.7858447e-03 -3.5356409e-03 -2.1513575e-03 -2.0210268e-03]\n"
     ]
    }
   ],
   "source": [
    "print(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('with', 0.9999999403953552),\n",
       " ('post', 0.026634827256202698),\n",
       " ('the', 0.005881853401660919),\n",
       " ('has', -0.04295908659696579)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.similar_by_vector(v1, topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words = word2vec.wv.most_similar('with')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('post', 0.0266348198056221), ('the', 0.005881853401660919), ('has', -0.04295909032225609)]\n"
     ]
    }
   ],
   "source": [
    "print(sim_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}