{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "from nltk import tokenize\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import Sequence\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Debug:\n",
    "    def __init__(self, debug_mode=True):\n",
    "        self.debug_mode = debug_mode\n",
    "        self.flag = {}\n",
    "\n",
    "    def log(self, target, flag=None):\n",
    "        if self.debug_mode:\n",
    "            if flag is None:\n",
    "                print(target)\n",
    "            else:\n",
    "                if flag in self.flag.keys():\n",
    "                    if self.flag[flag]:\n",
    "                        print(target)\n",
    "\n",
    "    def set_flag(self, flag: str, val: bool):\n",
    "        self.flag[flag] = val\n",
    "\n",
    "debug = Debug(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GeneratorExceptions(Exception):\n",
    "    \"\"\"\n",
    "    The Exception class for tracking all exceptions raised in data generator\n",
    "    Param\n",
    "        text: the displayed text\n",
    "    \"\"\"\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text\n",
    "\n",
    "class data_generator(Sequence):\n",
    "    def __init__(self, \n",
    "                 dataset_file_path : str=\"data/dataset/nysk.xml\", \n",
    "                 processed_dataset_path: str =\"data/processed_dataset/\",\n",
    "                 batch_size = 2,\n",
    "                 shuffle = True):\n",
    "        self.dataset_file_path = dataset_file_path\n",
    "        self.processed_dataset_path = processed_dataset_path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.file_list = []\n",
    "        self.encoder_input_data = []\n",
    "        self.decoder_input_data = []\n",
    "        self.decoder_target_data = []\n",
    "        self.char_list = []\n",
    "        self.characters_set = set()\n",
    "        self.tokens_count = 0\n",
    "        self.max_sequence_len = 0\n",
    "        self.token_index = None\n",
    "        self.input_texts = None\n",
    "        self.model = None\n",
    "        \n",
    "        self.epoch_count = 0\n",
    "        \n",
    "        self.validation_split=0.1\n",
    "    \n",
    "    def generate_char_dict(self):\n",
    "        \n",
    "        for i in string.ascii_letters:\n",
    "            self.characters_set.add(i)\n",
    "        \n",
    "        for i in \"1234567890\\n\\t.,!(){}\\\"\\' \":\n",
    "            self.characters_set.add(i)\n",
    "        \n",
    "        self.char_list = sorted(list(self.characters_set))\n",
    "        self.tokens_count = len(self.char_list)\n",
    "        self.token_index = dict([(char, i) for i, char in enumerate(self.characters_set)])\n",
    "\n",
    "        \n",
    "    def preprocess_data(self, override=False, max:int=3):\n",
    "        self.file_list = []\n",
    "        if os.path.isfile(self.dataset_file_path):\n",
    "            if not os.path.isdir(self.processed_dataset_path):\n",
    "                os.mkdir(self.processed_dataset_path)\n",
    "\n",
    "            with open(self.dataset_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                doc = ET.ElementTree(file=f)\n",
    "\n",
    "            root = doc.getroot()\n",
    "            \n",
    "            pos = 0\n",
    "            \n",
    "            for item in tqdm.tqdm(root):\n",
    "                if max != -1:\n",
    "                    if pos > max:\n",
    "                        break\n",
    "                    pos += 1\n",
    "                news_id = item.findtext('docid')\n",
    "                source = item.findtext('source')\n",
    "                url = item.findtext('url')\n",
    "                title = item.findtext('title')\n",
    "                summary = item.findtext('summary')\n",
    "                raw_text = item.findtext('text')\n",
    "\n",
    "                title = re.sub(r\"<.*>\", \"\", title)\n",
    "                title = re.sub(r\"\\W\", \"_\", title)\n",
    "                title = f\"{news_id}_{title[:10]}\"\n",
    "                \n",
    "                res = tokenize.sent_tokenize(raw_text)\n",
    "                sentences_count = len(res)\n",
    "                       \n",
    "                text = \"\"\n",
    "                \n",
    "                \n",
    "                for s in res:\n",
    "                    \n",
    "                    if len(s) > 200:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if len(s) > self.max_sequence_len:\n",
    "                            self.max_sequence_len = len(s)\n",
    "                        t_sentence = \"\"\n",
    "                        for c in s:\n",
    "                            if c in self.characters_set:\n",
    "                                t_sentence += c\n",
    "                        t_sentence += \"\\t\"\n",
    "                        text += t_sentence\n",
    "                \n",
    "                fp = f\"{self.processed_dataset_path}{title}_{sentences_count}.txt\"\n",
    "                \n",
    "                if not os.path.isfile(fp) or override:\n",
    "                    with open(fp, 'w', encoding='utf-8') as f:\n",
    "                        f.write(text)\n",
    "            self.max_sequence_len += 3\n",
    "            \n",
    "        else:\n",
    "            raise GeneratorExceptions(\"Path doesn't exist\")\n",
    "    \n",
    "    def generate_file_list(self, length:int=-1):\n",
    "        temp = os.listdir(self.processed_dataset_path)\n",
    "        self.file_list = []\n",
    "        t_list = []\n",
    "        for i in temp:\n",
    "            t_list.append(f\"{self.processed_dataset_path}{i}\")\n",
    "        self.file_list = t_list[1:length]\n",
    "    \n",
    "    def process_data(self, text):\n",
    "        input_texts = []\n",
    "        target_texts = []\n",
    "        \n",
    "        for i in range(0, len(text)-1):\n",
    "            input_t = f\"\\t{text[i]}\\n\"\n",
    "            target_t = f\"\\t{text[i+1]}\\n\"\n",
    "            input_texts.append(input_t)\n",
    "            target_texts.append(target_t)\n",
    "        \n",
    "        temp_encoder_input_data = np.zeros(\n",
    "            (len(input_texts), self.max_sequence_len, self.tokens_count), dtype=\"float32\"\n",
    "        )\n",
    "        temp_decoder_input_data = np.zeros(\n",
    "            (len(input_texts), self.max_sequence_len, self.tokens_count), dtype=\"float32\"\n",
    "        )\n",
    "        temp_decoder_target_data = np.zeros(\n",
    "            (len(input_texts), self.max_sequence_len, self.tokens_count), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "            for t, char in enumerate(input_text):\n",
    "                temp_encoder_input_data[i, t, self.token_index[char]] = 1.0\n",
    "                temp_encoder_input_data[i, t + 1 :, self.token_index[\" \"]] = 1.0\n",
    "            for t, char in enumerate(target_text):\n",
    "                # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "                temp_decoder_input_data[i, t, self.token_index[char]] = 1.0\n",
    "                if t > 0:\n",
    "                    # decoder_target_data will be ahead by one timestep\n",
    "                    # and will not include the start character.\n",
    "                    temp_decoder_target_data[i, t - 1, self.token_index[char]] = 1.0\n",
    "                    temp_decoder_input_data[i, t + 1 :, self.token_index[\" \"]] = 1.0\n",
    "                    temp_decoder_target_data[i, t:, self.token_index[\" \"]] = 1.0\n",
    "        \n",
    "        self.encoder_input_data = temp_encoder_input_data\n",
    "        self.decoder_input_data = temp_decoder_input_data\n",
    "        self.decoder_target_data = temp_decoder_target_data\n",
    "        \n",
    "        self.input_texts = input_texts\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        files = self.file_list[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        res = []\n",
    "        for fl in files:\n",
    "            with open(fl, 'r', encoding='utf-8') as dt:\n",
    "                text = dt.read()\n",
    "                temp_list = text.split(\"\\t\")\n",
    "                res.extend(temp_list)\n",
    "        \n",
    "        self.process_data(res)\n",
    "        return [self.encoder_input_data, self.decoder_input_data], self.decoder_target_data,\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.file_list)\n",
    "        model.save_weights(f\"ckpt/ckpt-{self.epoch_count}.hdf5\")\n",
    "        self.epoch_count += 1\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Testing data generator\n",
    "DataGenerator = data_generator()\n",
    "DataGenerator.generate_char_dict()\n",
    "print(DataGenerator.token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DataGenerator.preprocess_data(override=True, max=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(DataGenerator.tokens_count)\n",
    "print(DataGenerator.max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DataGenerator.generate_file_list()\n",
    "print(len(DataGenerator))\n",
    "print(len(DataGenerator.file_list))\n",
    "# print(temp_dg.__getitem__(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_model(num_encoder_tokens, num_decoder_tokens, latent_dim=256):\n",
    "      \n",
    "    encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs_, state_h, state_c = encoder(encoder_inputs)\n",
    "    \n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "    \n",
    "    decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "\n",
    "model = generate_model(num_encoder_tokens=DataGenerator.tokens_count, \n",
    "                       num_decoder_tokens=DataGenerator.tokens_count,\n",
    "                       latent_dim=latent_dim)\n",
    "\n",
    "model_name = \"Model\\SeqToSeq_Model\"\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(sys.getsizeof(model))\n",
    "# model.load_weights(\"ckpt/ckpt-0005.ckpt\")\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Don't execute this one for now\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "print(DataGenerator.token_index)\n",
    "DataGenerator.model = model\n",
    "\n",
    "checkpoint_path = \"ckpt/ckpt-{epoch:04d}.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    save_freq=DataGenerator.batch_size)\n",
    "\n",
    "model.fit(DataGenerator, \n",
    "          epochs=epochs, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"Model/test.weights.hdf5\")\n",
    "# keras.models.save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(DataGenerator: data_generator, input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, DataGenerator.tokens_count))\n",
    "    target_seq[0, 0, DataGenerator.token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        print(output_tokens)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > DataGenerator.max_sequence_len:\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, DataGenerator.tokens_count))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_model = generate_model(num_encoder_tokens=DataGenerator.tokens_count, \n",
    "                           num_decoder_tokens=DataGenerator.tokens_count,\n",
    "                           latent_dim=latent_dim)\n",
    "new_model.compile(\n",
    "    optimizer=\"rmsprop\", \n",
    "    loss=\"categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "new_model.summary()\n",
    "\n",
    "new_model.load_weights(\"Model/test.weights.hdf5\")\n",
    "# new_model.load_weights(\"ckpt/ckpt-0005.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "encoder_inputs = new_model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = new_model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = new_model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_5\")\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_6\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = new_model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = new_model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "reverse_input_char_index = dict((i, char) for char, i in DataGenerator.token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in DataGenerator.token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for seq_index in range(1):\n",
    "    print(seq_index)\n",
    "    DataGenerator.__getitem__(0)\n",
    "    input_seq = DataGenerator.encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(DataGenerator, input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", DataGenerator.input_texts[seq_index])\n",
    "    for i in decoded_sentence:\n",
    "        print(f\"{i}, {ord(i)}\")\n",
    "    print(\"Decoded sentence:\", decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}